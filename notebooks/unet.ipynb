{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header files\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import random\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "from collections import namedtuple\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the experiment produces same result on each run\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of Cityscapes class\n",
    "class Cityscapes(torch.utils.data.Dataset):\n",
    "    \"\"\"Cityscapes <http://www.cityscapes-dataset.com/> Dataset.\n",
    "    \n",
    "    **Parameters:**\n",
    "        - **root** (string): Root directory of dataset where directory 'leftImg8bit' and 'gtFine' or 'gtCoarse' are located.\n",
    "        - **split** (string, optional): The image split to use, 'train', 'test' or 'val' if mode=\"gtFine\" otherwise 'train', 'train_extra' or 'val'\n",
    "        - **mode** (string, optional): The quality mode to use, 'gtFine' or 'gtCoarse' or 'color'. Can also be a list to output a tuple with all specified target types.\n",
    "        - **transform** (callable, optional): A function/transform that takes in a PIL image and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        - **target_transform** (callable, optional): A function/transform that takes in the target and transforms it.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    CityscapesClass = namedtuple('CityscapesClass', ['name', 'id', 'train_id', 'category', 'category_id', 'has_instances', 'ignore_in_eval', 'color'])\n",
    "    classes = [\n",
    "        CityscapesClass('unlabeled',            0, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('ego vehicle',          1, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('rectification border', 2, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('out of roi',           3, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('static',               4, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('dynamic',              5, 255, 'void', 0, False, True, (111, 74, 0)),\n",
    "        CityscapesClass('ground',               6, 255, 'void', 0, False, True, (81, 0, 81)),\n",
    "        CityscapesClass('road',                 7, 0, 'flat', 1, False, False, (128, 64, 128)),\n",
    "        CityscapesClass('sidewalk',             8, 1, 'flat', 1, False, False, (244, 35, 232)),\n",
    "        CityscapesClass('parking',              9, 255, 'flat', 1, False, True, (250, 170, 160)),\n",
    "        CityscapesClass('rail track',           10, 255, 'flat', 1, False, True, (230, 150, 140)),\n",
    "        CityscapesClass('building',             11, 2, 'construction', 2, False, False, (70, 70, 70)),\n",
    "        CityscapesClass('wall',                 12, 3, 'construction', 2, False, False, (102, 102, 156)),\n",
    "        CityscapesClass('fence',                13, 4, 'construction', 2, False, False, (190, 153, 153)),\n",
    "        CityscapesClass('guard rail',           14, 255, 'construction', 2, False, True, (180, 165, 180)),\n",
    "        CityscapesClass('bridge',               15, 255, 'construction', 2, False, True, (150, 100, 100)),\n",
    "        CityscapesClass('tunnel',               16, 255, 'construction', 2, False, True, (150, 120, 90)),\n",
    "        CityscapesClass('pole',                 17, 5, 'object', 3, False, False, (153, 153, 153)),\n",
    "        CityscapesClass('polegroup',            18, 255, 'object', 3, False, True, (153, 153, 153)),\n",
    "        CityscapesClass('traffic light',        19, 6, 'object', 3, False, False, (250, 170, 30)),\n",
    "        CityscapesClass('traffic sign',         20, 7, 'object', 3, False, False, (220, 220, 0)),\n",
    "        CityscapesClass('vegetation',           21, 8, 'nature', 4, False, False, (107, 142, 35)),\n",
    "        CityscapesClass('terrain',              22, 9, 'nature', 4, False, False, (152, 251, 152)),\n",
    "        CityscapesClass('sky',                  23, 10, 'sky', 5, False, False, (70, 130, 180)),\n",
    "        CityscapesClass('person',               24, 11, 'human', 6, True, False, (220, 20, 60)),\n",
    "        CityscapesClass('rider',                25, 12, 'human', 6, True, False, (255, 0, 0)),\n",
    "        CityscapesClass('car',                  26, 13, 'vehicle', 7, True, False, (0, 0, 142)),\n",
    "        CityscapesClass('truck',                27, 14, 'vehicle', 7, True, False, (0, 0, 70)),\n",
    "        CityscapesClass('bus',                  28, 15, 'vehicle', 7, True, False, (0, 60, 100)),\n",
    "        CityscapesClass('caravan',              29, 255, 'vehicle', 7, True, True, (0, 0, 90)),\n",
    "        CityscapesClass('trailer',              30, 255, 'vehicle', 7, True, True, (0, 0, 110)),\n",
    "        CityscapesClass('train',                31, 16, 'vehicle', 7, True, False, (0, 80, 100)),\n",
    "        CityscapesClass('motorcycle',           32, 17, 'vehicle', 7, True, False, (0, 0, 230)),\n",
    "        CityscapesClass('bicycle',              33, 18, 'vehicle', 7, True, False, (119, 11, 32)),\n",
    "        CityscapesClass('license plate',        -1, 255, 'vehicle', 7, False, True, (0, 0, 142)),\n",
    "    ]\n",
    "\n",
    "    train_id_to_color = [c.color for c in classes if (c.train_id != -1 and c.train_id != 255)]\n",
    "    train_id_to_color.append([0, 0, 0])\n",
    "    train_id_to_color = np.array(train_id_to_color)\n",
    "    id_to_train_id = np.array([c.train_id for c in classes])\n",
    "\n",
    "    # init method\n",
    "    def __init__(self, root, split='train', transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): path of the directory which contains the required images and masks\n",
    "            split (string): can have value either 'train' or 'val'\n",
    "            transform (torchvision.transforms): transforms applied on the given image\n",
    "        \"\"\"\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.mode = 'gtFine'\n",
    "        self.target_type = 'semantic'\n",
    "        self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
    "        self.targets_dir = os.path.join(self.root, self.mode, split)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.split = split\n",
    "        self.images = []\n",
    "        self.targets = []\n",
    "\n",
    "        if split not in ['train', 'test', 'val']:\n",
    "            raise ValueError('Invalid split for mode! Please use split=\"train\", split=\"test\" or split=\"val\"')\n",
    "        if not os.path.isdir(self.images_dir) or not os.path.isdir(self.targets_dir):\n",
    "            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the specified \"split\" and \"mode\" are inside the \"root\" directory')\n",
    "        \n",
    "        for city in os.listdir(self.images_dir):\n",
    "            img_dir = os.path.join(self.images_dir, city)\n",
    "            target_dir = os.path.join(self.targets_dir, city)\n",
    "\n",
    "            for file_name in os.listdir(img_dir):\n",
    "            \n",
    "                self.images.append(os.path.join(img_dir, file_name))\n",
    "                target_name = '{}_{}'.format(file_name.split('_leftImg8bit')[0], self._get_target_suffix(self.mode, self.target_type))\n",
    "                self.targets.append(os.path.join(target_dir, target_name))\n",
    "\n",
    "    @classmethod\n",
    "    def encode_target(cls, target):\n",
    "        return cls.id_to_train_id[np.array(target)]\n",
    "\n",
    "    @classmethod\n",
    "    def decode_target(cls, target):\n",
    "        target[target == 255] = 19\n",
    "        return cls.train_id_to_color[target]\n",
    "\n",
    "    # getitem method\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a tuple of all target types if target_type is a list with more\n",
    "            than one item. Otherwise target is a json object if target_type=\"polygon\", else the image segmentation.\n",
    "        \"\"\"\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "        target = Image.open(self.targets[index])\n",
    "\n",
    "        # random crop\n",
    "        i, j, h, w = torchvision.transforms.RandomCrop.get_params(image, output_size=(512,512))\n",
    "        image = torchvision.transforms.functional.crop(image, i, j, h, w)\n",
    "        target = torchvision.transforms.functional.crop(target, i, j, h, w)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        target = torch.from_numpy(np.array(target, dtype='uint8'))\n",
    "        target = self.encode_target(target)\n",
    "        return image, target\n",
    "\n",
    "    # len method\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: number of images in the directory\n",
    "        \"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def _load_json(self, path):\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def _get_target_suffix(self, mode, target_type):\n",
    "        if target_type == 'instance':\n",
    "            return '{}_instanceIds.png'.format(mode)\n",
    "        elif target_type == 'semantic':\n",
    "            return '{}_labelIds.png'.format(mode)\n",
    "        elif target_type == 'color':\n",
    "            return '{}_color.png'.format(mode)\n",
    "        elif target_type == 'polygon':\n",
    "            return '{}_polygons.json'.format(mode)\n",
    "        elif target_type == 'depth':\n",
    "            return '{}_disparity.png'.format(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "train_image_transform = torchvision.transforms.Compose([\n",
    "  torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "  torchvision.transforms.RandomHorizontalFlip(),\n",
    "  torchvision.transforms.ToTensor(),\n",
    "  torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_target_transform = torchvision.transforms.Compose([\n",
    "  torchvision.transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "val_image_transform = torchvision.transforms.Compose([\n",
    "  torchvision.transforms.ToTensor(),\n",
    "  torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cityscapes dataset from google drive link\n",
    "train_dataset = Cityscapes(root=\"/content/drive/My Drive/cityscapes/\", split='train', transform=train_image_transform, target_transform=train_target_transform)\n",
    "val_dataset = Cityscapes(root=\"/content/drive/My Drive/cityscapes/\", split='val', transform=val_image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and val loaders for the corresponding datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=16)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Net(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=19):\n",
    "        \"\"\"\n",
    "        U-Net class.\n",
    "        Arguments:\n",
    "        ----------\n",
    "        in_channels: int\n",
    "            The number of input channels.\n",
    "        out_channels: int\n",
    "            The number of output channels.\n",
    "        \"\"\"\n",
    "        super(U_Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.up5 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor = 2),\n",
    "            nn.Conv2d(1024, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "\n",
    "        self.up_conv5 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace = True),\n",
    "        )\n",
    "\n",
    "        self.up4 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor = 2),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "\n",
    "        self.up_conv4 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace = True),\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor = 2),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "\n",
    "        self.up_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace = True),\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor = 2),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "\n",
    "        self.up_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace = True),\n",
    "        )\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor = 2),\n",
    "            nn.Conv2d(64, out_channels, 1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computation of the U-Net.\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "        inputs: a 4-th order tensor of size \n",
    "            [batch_size, in_channels, height, width]\n",
    "            Input to the U-Net.\n",
    "        Returns:\n",
    "        --------\n",
    "        outputs: a 4-th order tensor of size\n",
    "            [batch_size, out_channels, height, width]\n",
    "            Output of the U-Net.  \n",
    "        \"\"\"\n",
    "\n",
    "        # encoding path\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)        \n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = self.conv4(x3)\n",
    "        x5 = self.conv5(x4)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.up5(x5)\n",
    "        d5 = torch.cat((x4, d5), dim=1)     \n",
    "        d5 = self.up_conv5(d5)\n",
    "        \n",
    "        d4 = self.up4(d5)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.up_conv4(d4)\n",
    "\n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.up_conv3(d3)\n",
    "\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.up_conv2(d2)\n",
    "\n",
    "        d1 = self.final(d2)\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = U_Net()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), 1e-4, 0.9, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=255, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics, reference: https://github.com/VainF/DeepLabV3Plus-Pytorch/blob/master/metrics/stream_metrics.py\n",
    "class _StreamMetrics(object):\n",
    "    def __init__(self):\n",
    "        \"\"\" Overridden by subclasses \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update(self, gt, pred):\n",
    "        \"\"\" Overridden by subclasses \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_results(self):\n",
    "        \"\"\" Overridden by subclasses \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def to_str(self, metrics):\n",
    "        \"\"\" Overridden by subclasses \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Overridden by subclasses \"\"\"\n",
    "        raise NotImplementedError()      \n",
    "\n",
    "class StreamSegMetrics(_StreamMetrics):\n",
    "    \"\"\"\n",
    "    Stream Metrics for Semantic Segmentation Task\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.confusion_matrix = np.zeros((n_classes, n_classes))\n",
    "\n",
    "    def update(self, label_trues, label_preds):\n",
    "        for lt, lp in zip(label_trues, label_preds):\n",
    "            self.confusion_matrix += self._fast_hist( lt.flatten(), lp.flatten() )\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_str(results):\n",
    "        string = \"\\n\"\n",
    "        for k, v in results.items():\n",
    "            if k!=\"Class IoU\":\n",
    "                string += \"%s: %f\\n\"%(k, v)\n",
    "        \n",
    "        #string+='Class IoU:\\n'\n",
    "        #for k, v in results['Class IoU'].items():\n",
    "        #    string += \"\\tclass %d: %f\\n\"%(k, v)\n",
    "        return string\n",
    "\n",
    "    def _fast_hist(self, label_true, label_pred):\n",
    "        mask = (label_true >= 0) & (label_true < self.n_classes)\n",
    "        hist = np.bincount(\n",
    "            self.n_classes * label_true[mask].astype(int) + label_pred[mask],\n",
    "            minlength=self.n_classes ** 2,\n",
    "        ).reshape(self.n_classes, self.n_classes)\n",
    "        return hist\n",
    "\n",
    "    def get_results(self):\n",
    "        \"\"\"Returns accuracy score evaluation result.\n",
    "            - overall accuracy\n",
    "            - mean accuracy\n",
    "            - mean IU\n",
    "            - fwavacc\n",
    "        \"\"\"\n",
    "        hist = self.confusion_matrix\n",
    "        acc = np.diag(hist).sum() / hist.sum()\n",
    "        acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
    "        acc_cls = np.nanmean(acc_cls)\n",
    "        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "        mean_iu = np.nanmean(iu)\n",
    "        freq = hist.sum(axis=1) / hist.sum()\n",
    "        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
    "        cls_iu = dict(zip(range(self.n_classes), iu))\n",
    "\n",
    "        return {\n",
    "                \"Overall Acc\": acc,\n",
    "                \"Mean Acc\": acc_cls,\n",
    "                \"FreqW Acc\": fwavacc,\n",
    "                \"Mean IoU\": mean_iu,\n",
    "                \"Class IoU\": cls_iu,\n",
    "            }\n",
    "        \n",
    "    def reset(self):\n",
    "        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes average values\"\"\"\n",
    "    def __init__(self):\n",
    "        self.book = dict()\n",
    "\n",
    "    def reset_all(self):\n",
    "        self.book.clear()\n",
    "    \n",
    "    def reset(self, id):\n",
    "        item = self.book.get(id, None)\n",
    "        if item is not None:\n",
    "            item[0] = 0\n",
    "            item[1] = 0\n",
    "\n",
    "    def update(self, id, val):\n",
    "        record = self.book.get(id, None)\n",
    "        if record is None:\n",
    "            self.book[id] = [val, 1]\n",
    "        else:\n",
    "            record[0]+=val\n",
    "            record[1]+=1\n",
    "\n",
    "    def get_results(self, id):\n",
    "        record = self.book.get(id, None)\n",
    "        assert record is not None\n",
    "        return record[0] / record[1]\n",
    "\n",
    "\n",
    "# set-up metrics\n",
    "metrics = StreamSegMetrics(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_accuracy_list = []\n",
    "train_iou_list = []\n",
    "val_loss_list = []\n",
    "val_accuracy_list = []\n",
    "val_iou_list = []\n",
    "\n",
    "# training and val loop\n",
    "for epoch in range(0, 1000):\n",
    "\n",
    "  # train\n",
    "  metrics.reset()\n",
    "  model.train()\n",
    "  train_loss = 0.0\n",
    "  for step, (images, labels) in enumerate(train_loader):\n",
    "    \n",
    "    # if cuda\n",
    "    images = images.to(device, dtype=torch.float32)\n",
    "    labels = labels.to(device, dtype=torch.long)\n",
    "    labels = labels.squeeze(1)\n",
    "    \n",
    "    # get loss\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "\n",
    "    # metrics\n",
    "    preds = outputs.detach().max(dim=1)[1].cpu().numpy()\n",
    "    targets = labels.cpu().numpy()\n",
    "    metrics.update(targets, preds)\n",
    "\n",
    "  # update training_loss, training_accuracy and training_iou \n",
    "  train_loss = train_loss/float(len(train_loader))\n",
    "  train_loss_list.append(train_loss)\n",
    "  results = metrics.get_results()\n",
    "  train_accuracy = results[\"Overall Acc\"]\n",
    "  train_iou = results[\"Mean IoU\"]\n",
    "  train_accuracy_list.append(train_accuracy)\n",
    "  train_iou_list.append(train_iou)\n",
    "\n",
    "  \n",
    "  # evaluation code\n",
    "  metrics.reset()\n",
    "  model.eval()\n",
    "  val_loss = 0.0\n",
    "  for step, (images, labels) in enumerate(val_loader):\n",
    "    with torch.no_grad():\n",
    "\n",
    "      # if cuda\n",
    "      images = images.to(device, dtype=torch.float32)\n",
    "      labels = labels.to(device, dtype=torch.long)\n",
    "      labels = labels.squeeze(1)\n",
    "\n",
    "      # get loss\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "      val_loss += loss.item()\n",
    "\n",
    "      # metrics\n",
    "      preds = outputs.detach().max(dim=1)[1].cpu().numpy()\n",
    "      targets = labels.cpu().numpy()\n",
    "      metrics.update(targets, preds)\n",
    "\n",
    "  # update val_loss, val_accuracy and val_iou \n",
    "  val_loss = val_loss / float(len(val_loader))\n",
    "  val_loss_list.append(val_loss)\n",
    "  results = metrics.get_results()\n",
    "  val_accuracy = results[\"Overall Acc\"]\n",
    "  val_iou = results[\"Mean IoU\"] \n",
    "  val_accuracy_list.append(val_accuracy)\n",
    "  val_iou_list.append(val_iou)\n",
    "\n",
    "\n",
    "  print()\n",
    "  print(\"Epoch: \" + str(epoch))\n",
    "  print(\"Training Loss: \" + str(train_loss) + \"    Validation Loss: \" + str(val_loss))\n",
    "  print(\"Training Accuracy: \" + str(train_accuracy) + \"    Validation Accuracy: \" + str(val_accuracy))\n",
    "  print(\"Training mIoU: \" + str(train_iou) + \"    Validhation mIoU: \" + str(val_iou))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "e = []\n",
    "for index in range(0, 1000):\n",
    "  e.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(e, train_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(e, val_loss_list)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
